# -*- coding: utf-8 -*-
"""A2_MT21046_MT21135.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14qgdX4nGID8-3W_PW60TD0lr4s11A2SV
"""

# importing libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

#load the datasets
links = pd.read_csv("links.csv")
movies = pd.read_csv("movies.csv")
ratings = pd.read_csv("ratings.csv")
tags = pd.read_csv("tags.csv")

"""# Q1 Exploratory Data Analysis"""

links.info()

links.head(10)

movies.head(10)

movies.describe()

ratings.head(10)

ratings.describe()

tags.head(10)

tags.describe()

"""# Q1 (1) finding frequently occurring values in categorical features"""

#finding the frequency of each element in column movieId
frequent1 = movies['genres'].value_counts()
print(frequent1)
#printing the most frequent element in column movieId
print("\n")
print("The most frequent(most repeated) one is "+movies.genres.mode())

#finding the frequency of each element in column movieId
frequent2 = movies['title'].value_counts()
print(frequent2)
#printing the most frequent element in column movieId
print("\n")
print("The most frequent(most repeated) one is "+movies.title.mode())

#finding the frequency of each element in column tag
frequent3=tags['tag'].value_counts()
print(frequent3)
#printing the most frequent element in column tag
print("\n")
print("The most frequent(most repeated) one is "+ tags.tag.mode())

#finding the frequency of each element in column rating
frequent4=ratings['rating'].value_counts()
print(frequent4)
#printing the most frequent element in column rating
print("\n")
print("The most frequent one(most repeated) in is "+ str(ratings.rating.mode()))

"""# Q1 (2) Counting of NAN values per column"""

print("Count of Nan values column wise in movies")
columns = list(movies)
for col in columns:
    print("Count of Nan values in column "+col+" is "+str(movies[col].isna().sum()))

print("Count of Nan values column wise in links")
columns = list(links)
for col in columns:
    print("Count of Nan values in column "+col+" is "+str(links[col].isna().sum()))

print("Count of Nan values column wise in tags")
columns = list(tags)
for col in columns:
    print("Number of Nan values in column "+col+" is "+str(tags[col].isna().sum()))

print("Count of Nan values column wise in ratings")
columns = list(ratings)
for col in columns:
    print("Number of Nan values in column "+col+" is "+str(ratings[col].isna().sum()))

"""# Q1 (3) Plotting correlations between features"""

hm1 = sns.heatmap(movies.corr(), annot = True)
hm1.set(xlabel='\nMovies Details', ylabel='Movies Details', title = "Correlation matrix of movies data\n")
plt.show()

hm2 = sns.heatmap(ratings.corr(), annot = True)
hm2.set(xlabel='\nRatings Details', ylabel='Ratings Details', title = "Correlation matrix of ratings data\n")
plt.show()

hm3 = sns.heatmap(links.corr(), annot = True)
hm3.set(xlabel='\nLinks Details', ylabel='Links Details', title = "Correlation matrix of links data\n")
plt.show()

hm4 = sns.heatmap(tags.corr(), annot = True)
hm4.set(xlabel='\nTags Details', ylabel='Tags Details', title = "Correlation matrix of tags data\n")
plt.show()

"""# Q1 (4) Insights from the dataset"""

# Split title and release year in separate columns in movies dataframe. Convert year to timestamp.
movies['year'] = movies.title.str.extract("\((\d{4})\)", expand=True)
movies.year = pd.to_datetime(movies.year, format='%Y')
movies.year = movies.year.dt.year # As there are some NaN years, resulting type will be float (decimals)
movies.title = movies.title.str[:-7]

movies.head()

#Insight 1: Number of movies per year
movies_year = movies[['movieId', 'year']].groupby('year')
fig, ax1 = plt.subplots(figsize=(10,5))
ax1.plot(movies_year.year.first(), movies_year.movieId.nunique(), "g-o")
ax1.grid(None)
ax1.set_ylim(0,)
ax1.set_xlabel('Year')
ax1.set_ylabel('Number of movies released')
plt.title('Movies per year')
plt.show()

ins2=tags['tag'].value_counts()
ins2

#Insight 2
ins2.columns =['tag', 'count']
plt.figure(figsize=(20,5))
ins2.plot(x='tag', y='count')
plt.show()

#Insight 3:  ratings vs number of movies
movies_rating = ratings[['movieId', 'rating']].groupby('rating')
fig, ax1 = plt.subplots(figsize=(10,5))
ax1.plot(movies_rating.rating.first(), movies_rating.movieId.nunique(), "r-o")
ax1.grid(None)
ax1.set_ylim(0,)
ax1.set_xlabel('Rating')
ax1.set_ylabel('Number of movies released')
plt.title('Rating vs number of movies')
plt.show()

#Insight 4
#Disribution of user's ratings
ratings.rating.plot.hist(bins=50)
plt.title("Distribution of Users' Ratings")
plt.ylabel('Number of Ratings')
plt.xlabel('Rating (Out of 5)');

"""# Q2 Association rule mining"""

movies = pd.read_csv("movies.csv")

#merging movies and tag dataframes on movieid using inner join
rec_df1 = movies.merge(tags,on = 'movieId',how = 'inner')
rec_df1

#dropping tag, time stamp and genres from the above merged dataframe
rec_df1=rec_df1.drop(columns=['tag','timestamp','genres'])
rec_df1

#we are merging movies with ratings
rec_df2 = movies.merge(ratings,on = 'movieId',how = 'inner')
rec_df2

#now dropping rating, timestamp and genres.
rec_df2=rec_df2.drop(columns=['rating','timestamp','genres'])
rec_df2

#concatinating rec_df1 with rec_df2
rec_df1=pd.concat([rec_df1,rec_df2])
rec_df1

#removing the duplicates that might have crept in while concatinating
rec_df1 = rec_df1.drop_duplicates(['userId','title'])
rec_df1

#grouping by userid and title to finally get
rec_df1 = rec_df1.groupby(by = ["userId"])["title"].apply(list).reset_index()
rec_df1.head()

movie_list=rec_df1['title'].tolist()
movie_list[0]

movie=list(movies.movieId.unique())
len(movie)

from mlxtend.preprocessing import TransactionEncoder
te = TransactionEncoder()
te_ary = te.fit(movie_list).transform(movie_list)
rec_df1 = pd.DataFrame(te_ary, columns=te.columns_)

rec_df1

pip install mlxtend --upgrade

#finding frequent itemsets
from mlxtend.frequent_patterns.fpgrowth import fpgrowth
fpgrowth_frequent_itemsets = fpgrowth(rec_df1, min_support=0.095, use_colnames=True)
fpgrowth_frequent_itemsets.head()

fpgrowth_frequent_itemsets['itemsets'].apply(lambda x: len(x)).value_counts()

#finding rules from the generated frequent item sets
from mlxtend.frequent_patterns import association_rules
rules = association_rules(fpgrowth_frequent_itemsets, metric="lift", min_threshold=1)

rules

rules=rules.sort_values(ascending=False,by='lift')
rules

rules.to_csv('file1.csv')

def check1_anticident(film):
    emp=[]
    def find(x):
        for i in film:
            if i in x:
                return True
        return False
    df_item=rules[rules["antecedents"].apply(lambda x: find(x)) ]
    for i in df_item.consequents:
        emp.extend(list(i))
        if len(set(emp))>=4:
            return list(set(emp))
    return list(set(emp))

def check_anticident(film):
    df_item=rules[rules['antecedents'] == film]
    emp=[]
    for i in df_item.consequents:
        emp.extend(list(i))
        if len(set(emp))>=4:
            return list(set(emp))
    return list(set(emp))

def get_movie(film):
    length=check_anticident(film)
    if len(length)>=4:
        return length[:4]
    length.extend(check1_anticident(film))
    return length[:4]

#film={'Dances with Wolves (1990)', 'Aladdin (1992)', 'Batman (1989)'}
film={'Dances with Wolves (1990)'}

get_movie(film)

test=pd.read_csv("test.tsv", sep='\t')
test

inference=[]
for i in range(len(test)):
    film=set((test.movies.iloc[i]).split('\n'))
    l=len(film)
    film_emp=get_movie(film)
    inference.append("\n".join(film_emp))

test['Inference']=inference

test

Output=test.drop("recommendation", axis=1)
Output

Output.to_csv("Output.csv")

rules[rules["antecedents"].apply(lambda x: "Dances with Wolves (1990)" in str((x))) ].groupby(['antecedents', 'consequents'])[['lift']].max().sort_values(ascending=False,by='lift')

"""# Q3) Visualizing maximal frequent itemset"""

import networkx as nx
from mlxtend.frequent_patterns import fpmax
fpmax = fpmax(rec_df1, min_support=0.095, use_colnames=True)

fpmax

max_freq=[]
n=51655
for i in range(7):
    max_freq.append(list(fpmax.iloc[n-i].itemsets))

set1=[]
set2=[]
for i in max_freq:
    for j in range(len(i)):
        k=i[:j]+i[j+1:]
        set1.append(i)
        set2.append(k)

s1=[]
d1=[]
for i in set1:
    s1.append(frozenset(i))
for i in set2:
    d1.append(frozenset(i))

visual=pd.DataFrame({'Subset': s1,'Maximal frequent set': d1,})
visual.head()

visual['Subset']=visual["Subset"].apply(lambda x: frozenset(x) )
visual['Maximal frequent set']=visual["Maximal frequent set"].apply(lambda x: frozenset(x))
visual.head()

edges = nx.from_pandas_edgelist(visual,source='Subset',target='Maximal frequent set',edge_attr=None)
l=list(visual["Maximal frequent set"])
color_map = []
for g in edges.nodes():
    if g in l:
        color_map.append('red')
    else: 
        color_map.append('black')  
plt.subplots(figsize=(40,30))
pos = nx.planar_layout(edges)
nx.draw_networkx_nodes(edges, pos, node_size = 2000,alpha= 0.7,node_color = color_map)
nx.draw_networkx_edges(edges, pos, width = 6, alpha = 0.2, edge_color = 'green')
#nx.draw_networkx_labels(edges, pos, font_size = 25, font_family = 'FreeMono')
plt.grid()
plt.axis('off')
plt.tight_layout()
plt.show()